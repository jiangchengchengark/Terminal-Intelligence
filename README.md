#2025.3.4留档

终端智能不考虑算力限制和速度，准确率仍难以实现。
MobileAgent使用ocr+图标识别的方式提供了大量可选择的坐标，但是以qwen-vl-max为例，在这样密集繁多的坐标中，难以找到正确答案。
视觉模型进行操作时，只是依靠概率在行动，实际测试发现，如果当前页面出现立即下单，点击就送、同意等字眼，例如点外卖场景，视觉大模型经常会忽略当前商品是否是用户目标商品而继续进行。
在我的设想和调试中，以下构建可能是最佳方案：
决策模型：接收当前图片以及终极目标，判断当前状态需要做什么（返回上一页，输入、点击、什么都不做，已完成等）。
决策模型直接调用命令行工具，对于需要确定坐标位置的命令，使用目标检测智能体执行。
"""
目标检测智能体运行流程如下：
任务：从一系列映射中找到当前屏幕中某一个目标的坐标。
首先，图片经过区域检测，智能体判断后将搜索范围减小到目标所在的区域内。
然后，图片经过目标检测，根据所选区域，保留区域内的检测结果。
智能体从映射中找到对象的坐标，并返回。
"""
命令立即执行，再重新进入流程的开始决策模型当中。

把终端操作想象成马尔可夫的决策过程，仅考虑视觉模型去做这个事。如果专门训练一个语言-视觉-动作模型(VLA)，或许就会智能很多。

